{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c1554d",
   "metadata": {},
   "source": [
    "## Spark Overview and Setup\n",
    "\n",
    "Apache Spark is an open-source, distributed processing system used for big data workloads. [Pyspark](https://spark.apache.org/docs/latest/api/python/getting_started/install.html) is a Python package that provides bindings to Sparkâ€™s distributed machine learning library.\n",
    "                                                                                                                                                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6409c1-ca51-4be7-ae88-b85b7dfb4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stratify_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a7eac",
   "metadata": {},
   "source": [
    "A Spark session can be initialized with multiple configuration options such as\n",
    "`spark.driver.memory` which allocates memory for the current workers. More information on available Spark configuration parameters can be found [here](https://spark.apache.org/docs/3.5.1/configuration.html#content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9f98f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fd9699868e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set up Spark context ###\n",
    "conf = SparkConf()\n",
    "conf.set('spark.driver.memory','100g')\n",
    "conf.set(\"spark.task.cpus\", \"1\")\n",
    "conf.set('spark.sql.broadcastTimeout', 6000)\n",
    "conf.set('spark.log.level', 'FATAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afe57ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/23 13:18:44 WARN Utils: Your hostname, pa-dev resolves to a loopback address: 127.0.1.1; using 10.12.16.211 instead (on interface ens192)\n",
      "24/07/23 13:18:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/23 13:18:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting Spark log level to \"FATAL\".\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName=\"LBS\", conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc8d0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/balderama/Gitlab/python-and-validation/Python/wc_env/lib/python3.8/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42b0dd",
   "metadata": {},
   "source": [
    "## Data Partitioning\n",
    "\n",
    "Partitioning is only necessary if the current size of the data cannot be read in memory using conventional Python packages. Currently the walkingcalculatr (Python) functions have been verified to work on LBS data  (< 300 million rows) given a 300 GB Linux machine. As read/write operations have slower performance, the smallest number of N partitions is recommended. A helper script `stratify_functions.py` is included to help with partitioning the data.\n",
    "\n",
    "The `create_partition_from_parquet()` function ensures N partitions from a given LBS directory are created\n",
    "with device stratified partitions. Device stratification is required for walkingcalculatr (Python) functions as\n",
    "all records within a given device must be located within the same file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a010aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/home/balderama/Gitlab/walkingcalculatr/data-raw/synthetic_ness_data.csv.gz') # Location of data\n",
    "strat_path = Path('/home/balderama/Gitlab/python-and-validation/Python/test_path') # Location to save partitions\n",
    "county = 'ness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e05cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stratification for ness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|partition_number| count|\n",
      "+----------------+------+\n",
      "|               3|  4616|\n",
      "|               0|200101|\n",
      "|               1|199916|\n",
      "|               2|199880|\n",
      "+----------------+------+\n",
      "\n",
      "Number of rows by partition: None\n",
      "CPU times: user 993 ms, sys: 699 ms, total: 1.69 s\n",
      "Wall time: 17.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Creates N partitions of max size 200K\n",
    "clean_df = create_partitions_from_parquet(sqlc, \n",
    "                                        data_path = data_path,\n",
    "                                        strat_path = strat_path,\n",
    "                                        county = county, \n",
    "                                        part_size = 200E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7f9a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----------------+----------------+-------------------+----------------+\n",
      "| ID|        LATITUDE|        LONGITUDE|       TIMESTAMP|HORIZONTAL_ACCURACY|partition_number|\n",
      "+---+----------------+-----------------+----------------+-------------------+----------------+\n",
      "| id|        latitude|        longitude|  unix_timestamp|horizontal_accuracy|               0|\n",
      "|  1| 38.450496673584|-99.8636856079102|    1.625112e+12|                  0|               2|\n",
      "|  1|38.4529304504395|-99.9013519287109|1625112818064.63|                  0|               2|\n",
      "|  1| 38.450496673584|-99.8636856079102|1625113549804.33|                  0|               2|\n",
      "|  1|38.4529304504395|-99.9013519287109|1625114385342.17|                  0|               2|\n",
      "|  1| 38.452751159668|-99.9047470092773|1625115061581.34|                  0|               2|\n",
      "|  1|38.4527587890625|-99.9047622680664| 1625116059785.8|                  0|               2|\n",
      "|  1| 38.452808380127|-99.9047622680664| 1625116848573.4|                  0|               2|\n",
      "|  1|38.4527587890625|-99.9047622680664|1625118794775.65|                  0|               2|\n",
      "|  1| 38.452808380127|-99.9047622680664|1625119856917.89|                  0|               2|\n",
      "|  1| 38.452751159668|-99.9047470092773|1625120656477.34|                  0|               2|\n",
      "|  1| 38.452808380127|-99.9047622680664|1625121483712.82|                  0|               2|\n",
      "|  1|38.4528198242188|-99.9047470092773|1625122252396.69|                  0|               2|\n",
      "|  1|38.4528617858887|-99.9048385620117|1625124762087.77|                  0|               2|\n",
      "|  1| 38.453971862793|   -99.9052734375|1625125422610.37|                  0|               2|\n",
      "|  1|38.4538688659668|-99.9052810668945|1625126234469.75|                  0|               2|\n",
      "|  1|38.4538803100586|-99.9052505493164|1625126891254.19|                  0|               2|\n",
      "|  1| 38.450496673584|-99.8636856079102|1625127552735.44|                  0|               2|\n",
      "|  1|38.4575614929199|-99.9008483886719|1625133281250.16|                  0|               2|\n",
      "|  1|38.4575386047363|-99.9009017944336|1625133937597.95|                  0|               2|\n",
      "+---+----------------+-----------------+----------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5d6bf",
   "metadata": {},
   "source": [
    "Data can be written to local memory using `write.parquet()` or `write.csv()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f65313",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Save by partition number to directory file_path\n",
    "file_path = str(strat_path / f\"lbs_{county}_device_stratified\")\n",
    "\n",
    "clean_df.write.option(\"header\", True)\\\n",
    "         .partitionBy('partition_number')\\\n",
    "         .mode(\"overwrite\")\\\n",
    "         .option(\"compression\", \"gzip\")\\\n",
    "         .csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27f028",
   "metadata": {},
   "source": [
    "A Spark session should be disconnected once spark is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End spark\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd861874",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8c37e",
   "metadata": {},
   "source": [
    "Currently, this notebook has been tested and verified on a MacOS system. During testing on a Windows machine, a common error was found involving Pyspark installation on Windows. A Hadoop environment error appears when running writing the partitions to memory using `spark.write.csv()`. Additional steps may be required to enable write operations using Pyspark on Windows, with some additional [resources](https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems) outlined to troubleshoot further. As of August 2024, further testing is needed to run `Stratify_Data.ipynb` a Windows machine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wc_env",
   "language": "python",
   "name": "wc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
