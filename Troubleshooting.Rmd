---
  title: "Troubleshooting"
  date: '`r paste("First created on June 01, 2023. Updated on", Sys.Date())`'
  output:
  html_document: 
    df_print: kable
---
```{r echo = F}
knitr::opts_knit$set(root.dir = dirname(dirname(rstudioapi::getSourceEditorContext()$path)))

# no scientific notation
options(scipen = 999)
```

## Updating R session temp directory

When working with files, R uses a default `/tmp` directory but can be updated
to another scratch directory to avoid memory issues (default limit for `/tmp` is 9 Gb).
```{r eval = F}
Sys.setenv(R_SESSION_TMPDIR = "/your/temp/directory/")
```

## Insufficient Memory for Large Data Extraction

For reference, `walkingcalculatr` functions have been tested for files (or partitions) up to 200M rows
on a 64 Gb Linux machine and 800M rows in an 250 Gb HPC environment. Packages like `data.table` have data limitations as well at 2^31 rows.

There exists some options when working with data to large to execute `walkingcalculatr` with current memory.

1. Data wrangling, de-duplication and/or subsetting on the raw data (using Spark)
2. Partitioning large dataset into smaller executable partitions (see [Spark Overview](Spark_Overview.Rmd))
3. Running code in a high performance computing (HPC) environment
4. Use functions like [open_dataset()](https://arrow.apache.org/docs/r/articles/dataset.html) in `arrow` to circumvent limitations

## No unix time format available (UTC to Unix time conversion)

If UNIX time is not available within the location based services (LBS) data set, a user can
convert their time stamp to UNIX time format using the base R's `as.POSIXct()`. An example of converting 10,000 sample timestamps (UTC) to UNIX time is shown.

```{r}
ts <- as.POSIXct(runif(1e4) * unclass(Sys.time()), "UTC")
head(ts)
```

Ensure that the character format for your timestamp matches the format given in `format`.
```{r}
converted_ts <- as.numeric(as.POSIXct(ts))
head(converted_ts)
```

For larger datasets, the `fasttime::fastPOSIXct()` function can be used to speed up execution time.
```{r}
converted_fast_ts <- as.numeric(fasttime::fastPOSIXct(ts))
head(converted_fast_ts)
```

## GEOID is being read incorrectly

A census tract GEOID will always have 11 characters, but reading in an existing walking trip file with mapped census tract GEOIDs may inherently remove leading 0s because it recognizes the GEOID as numeric instead of a character string. As a result, the GEOID will have != 11 characters and thus will not be correct. To remedy this, you can either classify the GEOID column as a character while reading the file in `data.table::fread()`. ***NOTE: For pings not in a census tract or in a walkable area, the GEOID should always be `NA` with a character count of 0.***

However, if any LBS ping can be mapped to more than one GEOID, the GEOID column will always be recognized as a character string due to the addition of the semicolon.

## Accuracy Filtering

Depending on the LBS vendor, a variable may be provided which typically
gives a vertical and/or horizontal accuracy of location in meters. Ensure that **horizontal accuracy** is selected.
